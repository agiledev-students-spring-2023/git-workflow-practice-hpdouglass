# Git Practice

**Abstracts written by ChatGPT fool scientists**

Link: <https://www.nature.com/articles/d41586-023-00056-7>

There is no doubt that ChatGPT is changing our world very rapidly. Tedious tasks can now be automated away very quickly, with only a quick read-through to make sure the AI hasn't made any glaring factual errors. This article talks a lot about detecting AI output compared to human output, saying that an algorithm was only able to detect 66% of the AI output.

I, however, am not convinced that checking AI output is something we need to be concerned about. If AI can make our lives easier, why shouldn't it? There are of course legal boundaries that have yet to be explored, but as for ethical ones? I don't see why scientists and researchers shouldn't use a tool to help them compose their articles if it makes their lives easier. So long as they ensure the facts of the article are correct, I don't see any problem.

In elementary school, I was always told that we "wouldn't have a calculator around" every time we wanted to make a calculation. Yet now, we do, in the form of smartphones. We have adapted to technology before, and I don't see why ChatGPT should be any different.

---

**Comment by Kevin Park (cp3111)** What an interesting read! It is concerning that the dawn of AI technology already has qualified researchers unable to distinguish whether an abstract is created by a colleague or AI. I wonder how academia--along with the rest of the world--will battle malicious and unethical uses of ChatGPT and future commercially-available AIs to come.